{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Natural Language Processing**\n",
        " * NLP is an interdesciplinary subfield of linguistics, compute science , and artificial intelligence concerned with the interactions between computers and human language, or how to program computers to process and analyze large amount of human language data."
      ],
      "metadata": {
        "id": "1GZWVKCBg3OI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Libraries for NLP**\n",
        " * NLTK - Natural language tool kit\n",
        " * Textblob"
      ],
      "metadata": {
        "id": "xXRryuy_ifIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**\n",
        " * Word Tokenization\n",
        " * Sent Tokenization\n",
        " * Blankline Tokenization"
      ],
      "metadata": {
        "id": "STEjyBJYircl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmhRm_dqjTpB",
        "outputId": "8863adc5-fcab-4bd2-b432-a22845ffaf33"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wZy1hd1Pgsyn"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIa1Ixspk0PV",
        "outputId": "48a05e19-eb71-4c9b-ebc3-5c05c55d494a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Word Tokenization"
      ],
      "metadata": {
        "id": "shqTL3cTk-zA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AI = \"Hello Mr. John, how are you doing today? The weather is great, and Python is awesome Programming Language. The sky is blue. You shouldn't eat sugar. \""
      ],
      "metadata": {
        "id": "PWOskBfIjN3P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "AI_token = word_tokenize(AI)\n",
        "print(AI_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wDuvFxsj1y6",
        "outputId": "838cb91a-e644-4b69-fd9c-ff4b20aa09ba"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Mr.', 'John', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', 'Programming', 'Language', '.', 'The', 'sky', 'is', 'blue', '.', 'You', 'should', \"n't\", 'eat', 'sugar', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Sentence Tokenization"
      ],
      "metadata": {
        "id": "cvdbdRO4lBtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "AI_token_sent = sent_tokenize(AI)\n",
        "print(AI_token_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cjn3QmLikueK",
        "outputId": "d126d19f-cce2-42cc-d2b1-1cc1f1f93020"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello Mr. John, how are you doing today?', 'The weather is great, and Python is awesome Programming Language.', 'The sky is blue.', \"You shouldn't eat sugar.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Blankline Tokenization"
      ],
      "metadata": {
        "id": "8nN4SieUlTgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import blankline_tokenize\n",
        "AI_token_blank = blankline_tokenize(AI)\n",
        "print(AI_token_blank)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sE0QgfulLK_",
        "outputId": "97374030-3d5c-4bf5-f9cb-242dd69d4b8b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Hello Mr. John, how are you doing today? The weather is great, and Python is awesome Programming Language. The sky is blue. You shouldn't eat sugar. \"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Frequency Distribution"
      ],
      "metadata": {
        "id": "ZnkzsBnvmXC2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "fdist = FreqDist()"
      ],
      "metadata": {
        "id": "TcLLXkrUlc-z"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in AI_token:\n",
        "  fdist[word.lower()] += 1\n",
        "\n",
        "fdist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFrDl-tzmj0B",
        "outputId": "02f828b1-7d05-4a70-8f89-e7a0be456d86"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'is': 3, '.': 3, ',': 2, 'you': 2, 'the': 2, 'hello': 1, 'mr.': 1, 'john': 1, 'how': 1, 'are': 1, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhI7rCPPm2Ua",
        "outputId": "b1192ade-afad-489e-8eb1-d5f56c783107"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['hello', 'mr.', 'john', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'the', 'weather', 'is', 'great', 'and', 'python', 'awesome', 'programming', 'language', '.', 'sky', 'blue', 'should', \"n't\", 'eat', 'sugar'])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(fdist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e60X-LFonOgp",
        "outputId": "ffcb5271-9ada-4d63-a6b5-4e45029c0350"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "26"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist_top_10 = fdist.most_common(10)\n",
        "fdist_top_10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXkel2VAnD2U",
        "outputId": "1e32152e-cf75-4b1d-ffd1-d6d26bbe7ea3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('is', 3),\n",
              " ('.', 3),\n",
              " (',', 2),\n",
              " ('you', 2),\n",
              " ('the', 2),\n",
              " ('hello', 1),\n",
              " ('mr.', 1),\n",
              " ('john', 1),\n",
              " ('how', 1),\n",
              " ('are', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Three Types of token**\n",
        " * Biagram :- token of two consecutive written words\n",
        " * Trigram :- token of three consecutive written words\n",
        " * Ngram :- token of n-number of consecutive written words"
      ],
      "metadata": {
        "id": "z8yZL3oJn5NG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string = 'The most beautiful things in the world cannot be seen or even touched, they must be felt with the heart'\n",
        "quotes_token = nltk.word_tokenize(string)\n",
        "print(quotes_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rgLAnDrnnLlc",
        "outputId": "8f5ad25d-936d-4d30-fbc8-e098a6886c6b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'most', 'beautiful', 'things', 'in', 'the', 'world', 'can', 'not', 'be', 'seen', 'or', 'even', 'touched', ',', 'they', 'must', 'be', 'felt', 'with', 'the', 'heart']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quotes_bigrams = list(nltk.bigrams(quotes_token))\n",
        "quotes_bigrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPEkqFTmomU-",
        "outputId": "538570b0-0d81-4811-a977-953a3f775962"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'most'),\n",
              " ('most', 'beautiful'),\n",
              " ('beautiful', 'things'),\n",
              " ('things', 'in'),\n",
              " ('in', 'the'),\n",
              " ('the', 'world'),\n",
              " ('world', 'can'),\n",
              " ('can', 'not'),\n",
              " ('not', 'be'),\n",
              " ('be', 'seen'),\n",
              " ('seen', 'or'),\n",
              " ('or', 'even'),\n",
              " ('even', 'touched'),\n",
              " ('touched', ','),\n",
              " (',', 'they'),\n",
              " ('they', 'must'),\n",
              " ('must', 'be'),\n",
              " ('be', 'felt'),\n",
              " ('felt', 'with'),\n",
              " ('with', 'the'),\n",
              " ('the', 'heart')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quotes_trigrams = list(nltk.trigrams(quotes_token))\n",
        "quotes_trigrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2q-4Prhot_c",
        "outputId": "09066e92-3668-46bd-bc26-7a184e05dad2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'most', 'beautiful'),\n",
              " ('most', 'beautiful', 'things'),\n",
              " ('beautiful', 'things', 'in'),\n",
              " ('things', 'in', 'the'),\n",
              " ('in', 'the', 'world'),\n",
              " ('the', 'world', 'can'),\n",
              " ('world', 'can', 'not'),\n",
              " ('can', 'not', 'be'),\n",
              " ('not', 'be', 'seen'),\n",
              " ('be', 'seen', 'or'),\n",
              " ('seen', 'or', 'even'),\n",
              " ('or', 'even', 'touched'),\n",
              " ('even', 'touched', ','),\n",
              " ('touched', ',', 'they'),\n",
              " (',', 'they', 'must'),\n",
              " ('they', 'must', 'be'),\n",
              " ('must', 'be', 'felt'),\n",
              " ('be', 'felt', 'with'),\n",
              " ('felt', 'with', 'the'),\n",
              " ('with', 'the', 'heart')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quotes_ngrams = list(nltk.ngrams(quotes_token , 5))\n",
        "quotes_ngrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ji52UBXo7lB",
        "outputId": "4729498d-45dd-4af8-8539-7c765e8743dd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('The', 'most', 'beautiful', 'things', 'in'),\n",
              " ('most', 'beautiful', 'things', 'in', 'the'),\n",
              " ('beautiful', 'things', 'in', 'the', 'world'),\n",
              " ('things', 'in', 'the', 'world', 'can'),\n",
              " ('in', 'the', 'world', 'can', 'not'),\n",
              " ('the', 'world', 'can', 'not', 'be'),\n",
              " ('world', 'can', 'not', 'be', 'seen'),\n",
              " ('can', 'not', 'be', 'seen', 'or'),\n",
              " ('not', 'be', 'seen', 'or', 'even'),\n",
              " ('be', 'seen', 'or', 'even', 'touched'),\n",
              " ('seen', 'or', 'even', 'touched', ','),\n",
              " ('or', 'even', 'touched', ',', 'they'),\n",
              " ('even', 'touched', ',', 'they', 'must'),\n",
              " ('touched', ',', 'they', 'must', 'be'),\n",
              " (',', 'they', 'must', 'be', 'felt'),\n",
              " ('they', 'must', 'be', 'felt', 'with'),\n",
              " ('must', 'be', 'felt', 'with', 'the'),\n",
              " ('be', 'felt', 'with', 'the', 'heart')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steeming :- Changes to the token**\n",
        "  * Normalize a word to its base form or root form\n",
        "  * Affection, Affects, Affections, Affected, Affecting => Affect\n",
        "  * stemming algorithm works by cutting ends or beginning of the words taking into account most common word."
      ],
      "metadata": {
        "id": "peLlNiuupjkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "pst = PorterStemmer()\n",
        "pst.stem('having')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "wlnkztT8pC6G",
        "outputId": "dbd1b774-255d-40db-a548-8cd1329321b2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'have'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word_to_stem = ['give' , 'giving' , 'given' , 'gave']\n",
        "#word_to_stem = ['argue', ' argued' , 'argues' , 'arguing']\n",
        "word_to_stem = ['eating' , 'eats' , 'eat' , 'ate' , 'eaten']\n",
        "\n",
        "for word in word_to_stem:\n",
        "  print(word , '-' , pst.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0b2z_tDqK_O",
        "outputId": "7114cd65-b346-4169-b6df-aa7c92f81588"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating - eat\n",
            "eats - eat\n",
            "eat - eat\n",
            "ate - ate\n",
            "eaten - eaten\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "lst = LancasterStemmer()\n",
        "\n",
        "for word in word_to_stem:\n",
        "  print(word , '-' , lst.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeZC1NUGqhJl",
        "outputId": "e73f4383-4f4f-4afa-b755-da238f638a65"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating - eat\n",
            "eats - eat\n",
            "eat - eat\n",
            "ate - at\n",
            "eaten - eat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Lancaster Stemmer is more aggresive then Porter Stemmmer\n",
        "* Snowball Stemmer is a update in porter stemmer with multi language supports"
      ],
      "metadata": {
        "id": "Wxts0cNDrsbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "sbst = SnowballStemmer('english')\n",
        "\n",
        "for word in word_to_stem:\n",
        "  print(word , '-' , sbst.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v055vbKrWHo",
        "outputId": "66a49388-959b-4d54-8b9c-5012f4824210"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating - eat\n",
            "eats - eat\n",
            "eat - eat\n",
            "ate - ate\n",
            "eaten - eaten\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lemmatization:- In case when stemming don't results correctly for example fish, fishes, fisherman. I uses morphological analysis of words which may turn it into fish using stemming**"
      ],
      "metadata": {
        "id": "W7OgP1aKt3Vg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_stem = ['study' , 'studies' , 'studying']\n",
        "\n",
        "pst = PorterStemmer()\n",
        "lst = LancasterStemmer()\n",
        "sbst = SnowballStemmer('english')\n",
        "\n",
        "for word in word_to_stem:\n",
        "  print(word , '-' , sbst.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np5rSw2QsKCK",
        "outputId": "8ae43c13-9da3-4ba1-e169-95be08306565"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "study - studi\n",
            "studies - studi\n",
            "studying - studi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What does lemmatization do?**\n",
        " * Group together different infected forms of word, called lemma.\n",
        " * Somehow similar to stemming, as it maps several word into common root.\n",
        " * Output of lemmatization is a proper word\n",
        " * For example, a lemmatizer can map gone, going and went into go."
      ],
      "metadata": {
        "id": "unSbuPuXwEgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEZXoK2Evrq9",
        "outputId": "2200f80b-6d6d-49e4-8b87-a949d0e3cef4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJCOASSFvvBW",
        "outputId": "9c488436-40b1-438c-b8bd-11245c6088d4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "word_len = WordNetLemmatizer()\n",
        "word_len.lemmatize('corpora')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xszERV8QvMkB",
        "outputId": "01b40089-5863-40cf-b8ff-e2386867ceb5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'corpus'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_lem = ['study' , 'studies' , 'studying']\n",
        "\n",
        "for word in word_to_lem:\n",
        "  print(word , '-' , word_len.lemmatize(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_R4T3aEvmQc",
        "outputId": "c61aefc0-b9c0-4e6a-d8d3-6c6476d2b385"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "study - study\n",
            "studies - study\n",
            "studying - studying\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stop words**\n",
        " * Several word in english such as I,at,or,begin,got,know,various etc. which areusefull in making sentences but these are not that useful in NLP.\n",
        " * So these are called stopwords."
      ],
      "metadata": {
        "id": "oAI3PRWvxBdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQfrV-45xoj4",
        "outputId": "f09f1a6e-69b8-46e3-fc27-c7c3acc4a75f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfZg_3K4v-7l",
        "outputId": "b972145e-11a8-4883-ec97-7a776504a7fc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(stopwords.words('english')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W8N6lr_xlbY",
        "outputId": "8950a896-9142-46b5-8476-e0d197e34a67"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "179\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Punctuations**"
      ],
      "metadata": {
        "id": "NQZG7sg4yaRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "punctuations = re.compile(r'[-.?!,:;()\\'|0-9]')"
      ],
      "metadata": {
        "id": "7bQLkSfax0Y-"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "post_punctuations = []\n",
        "for word in AI_token:\n",
        "  word = punctuations.sub('_',word)\n",
        "  post_punctuations.append(word)"
      ],
      "metadata": {
        "id": "XabUc8sjyuuO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(post_punctuations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNdrer7wzEk_",
        "outputId": "d6c9216c-1585-45af-b46d-6fafea1337a0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Mr_', 'John', '_', 'how', 'are', 'you', 'doing', 'today', '_', 'The', 'weather', 'is', 'great', '_', 'and', 'Python', 'is', 'awesome', 'Programming', 'Language', '_', 'The', 'sky', 'is', 'blue', '_', 'You', 'should', 'n_t', 'eat', 'sugar', '_']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parts of Speech (POS Tags)**\n",
        "  * Generally speaking gramatical types of words like noun, verb, adverb, adjectives etc.\n",
        "  * A word can have more then one part of speech based upon context it is used\n",
        "  * Ex. \"Google Someyhing on internet\" , Here google is used as a verb although it is a noun.\n",
        "  * these are some sort of ambiquities or difficulties which makes things complicated.\n",
        "  *****\n",
        "  POS Tags are used to describe whether a word is a noun, an adjective , a proper noun , singular, plural , verb , adverb, symbol etc."
      ],
      "metadata": {
        "id": "_8UlB3y3z4gT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0gOktV-1cpY",
        "outputId": "5ed966f4-9717-417d-df31-c947013ea7b7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Abbreviation \tMeaning**\n",
        "* **CC** \tcoordinating conjunction\n",
        "* **CD** \tcardinal digit\n",
        "* **DT** \tdeterminer\n",
        "* **EX** \texistential there\n",
        "* **FW** \tforeign word\n",
        "* **IN** \tpreposition/subordinating conjunction\n",
        "* **JJ** \tThis NLTK POS Tag is an adjective (large)\n",
        "* **JJR** \tadjective, comparative (larger)\n",
        "* **JJS** \tadjective, superlative (largest)\n",
        "* **LS** \tlist market\n",
        "* **MD** \tmodal (could, will)\n",
        "* **NN** \tnoun, singular (cat, tree)\n",
        "* **NNS** \tnoun plural (desks)\n",
        "* **NNP** \tproper noun, singular (sarah)\n",
        "* **NNPS** \tproper noun, plural (indians or americans)\n",
        "* **PDT** \tpredeterminer (all, both, half)\n",
        "* **POS** \tpossessive ending (parent\\ ‘s)\n",
        "* **PRP** \tpersonal pronoun (hers, herself, him, himself)\n",
        "* **PRP$** \tpossessive pronoun (her, his, mine, my, our )\n",
        "* **RB** \tadverb (occasionally, swiftly)\n",
        "* **RBR** \tadverb, comparative (greater)\n",
        "* **RBS** \tadverb, superlative (biggest)\n",
        "* **RP** \tparticle (about)\n",
        "* **TO** \tinfinite marker (to)\n",
        "* **UH** \tinterjection (goodbye)\n",
        "* **VB** \tverb (ask)\n",
        "* **VBG** \tverb gerund (judging)\n",
        "* **VBD** \tverb past tense (pleaded)\n",
        "* **VBN** \tverb past participle (reunified)\n",
        "* **VBP** \tverb, present tense not 3rd person singular(wrap)\n",
        "* **VBZ** \tverb, present tense with 3rd person singular (bases)\n",
        "* **WDT** \twh-determiner (that, what)\n",
        "* **WP** \twh- pronoun (who)\n",
        "* **WRB** \twh- adverb (how)"
      ],
      "metadata": {
        "id": "ODZETcH017er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'Joy is a natural when it comes to a singing.'\n",
        "\n",
        "s_token = word_tokenize(sent)\n",
        "print(s_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD4DpxsTzGYH",
        "outputId": "08abfdec-b386-4354-c344-77609981f52c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Joy', 'is', 'a', 'natural', 'when', 'it', 'comes', 'to', 'a', 'singing', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in s_token:\n",
        "  print(nltk.pos_tag([token]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oouSSNtK1P4n",
        "outputId": "d38a3bc8-b8e6-473f-8fec-e327e7efd541"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Joy', 'NN')]\n",
            "[('is', 'VBZ')]\n",
            "[('a', 'DT')]\n",
            "[('natural', 'JJ')]\n",
            "[('when', 'WRB')]\n",
            "[('it', 'PRP')]\n",
            "[('comes', 'VBZ')]\n",
            "[('to', 'TO')]\n",
            "[('a', 'DT')]\n",
            "[('singing', 'VBG')]\n",
            "[('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in AI_token:\n",
        "  print(nltk.pos_tag([token]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiPijp1S1Zqy",
        "outputId": "1c8dccb2-1ef6-419e-8e47-f7ccbac2dacd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hello', 'NN')]\n",
            "[('Mr.', 'NNP')]\n",
            "[('John', 'NNP')]\n",
            "[(',', ',')]\n",
            "[('how', 'WRB')]\n",
            "[('are', 'VBP')]\n",
            "[('you', 'PRP')]\n",
            "[('doing', 'VBG')]\n",
            "[('today', 'NN')]\n",
            "[('?', '.')]\n",
            "[('The', 'DT')]\n",
            "[('weather', 'NN')]\n",
            "[('is', 'VBZ')]\n",
            "[('great', 'JJ')]\n",
            "[(',', ',')]\n",
            "[('and', 'CC')]\n",
            "[('Python', 'NN')]\n",
            "[('is', 'VBZ')]\n",
            "[('awesome', 'NN')]\n",
            "[('Programming', 'VBG')]\n",
            "[('Language', 'NN')]\n",
            "[('.', '.')]\n",
            "[('The', 'DT')]\n",
            "[('sky', 'NN')]\n",
            "[('is', 'VBZ')]\n",
            "[('blue', 'NN')]\n",
            "[('.', '.')]\n",
            "[('You', 'PRP')]\n",
            "[('should', 'MD')]\n",
            "[(\"n't\", 'RB')]\n",
            "[('eat', 'NN')]\n",
            "[('sugar', 'NN')]\n",
            "[('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named Entity Recognition**\n",
        "* Naming such as - \n",
        "  * Movie\n",
        "  * monetary value\n",
        "  * organization\n",
        "  * location\n",
        "  * Quantities\n",
        "  * person from a text\n"
      ],
      "metadata": {
        "id": "Xh3hNt_e2Oq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NE_sent = \"Google's CEO Sundar Pichai introduced the new pixel at Minnesota Roi Centre Event\"\n",
        "NE_token = word_tokenize(NE_sent)\n",
        "NE_tags = nltk.pos_tag(NE_token)\n",
        "NE_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubZr6Ofk2HwJ",
        "outputId": "f6571905-2685-4d08-fb99-b95921ff4d48"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Google', 'NNP'),\n",
              " (\"'s\", 'POS'),\n",
              " ('CEO', 'NNP'),\n",
              " ('Sundar', 'NNP'),\n",
              " ('Pichai', 'NNP'),\n",
              " ('introduced', 'VBD'),\n",
              " ('the', 'DT'),\n",
              " ('new', 'JJ'),\n",
              " ('pixel', 'NN'),\n",
              " ('at', 'IN'),\n",
              " ('Minnesota', 'NNP'),\n",
              " ('Roi', 'NNP'),\n",
              " ('Centre', 'NNP'),\n",
              " ('Event', 'NNP')]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('maxent_ne_chunker')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxWm9_wK3Wk7",
        "outputId": "502bd045-fc91-48a1-b11e-4d5652d23ce3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmEbBFL43ZZR",
        "outputId": "f906b829-a9d7-490b-c6f0-bddef20b4b99"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ne_chunk\n",
        "NE_NER = ne_chunk(NE_tags)\n",
        "print(NE_NER)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loBN0HsC3CC5",
        "outputId": "5db374a9-5081-483b-d27e-ca0585bb688f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (GPE Google/NNP)\n",
            "  's/POS\n",
            "  (ORGANIZATION CEO/NNP Sundar/NNP Pichai/NNP)\n",
            "  introduced/VBD\n",
            "  the/DT\n",
            "  new/JJ\n",
            "  pixel/NN\n",
            "  at/IN\n",
            "  (ORGANIZATION Minnesota/NNP Roi/NNP Centre/NNP)\n",
            "  Event/NNP)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NER Entities List**\n",
        "* Facility\n",
        "* Location\n",
        "* Organization\n",
        "* Person\n",
        "* GEO-Socio-Political Group\n",
        "* GEO-Political Entity"
      ],
      "metadata": {
        "id": "nIK2zCYu3mPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Syntax**\n",
        "* Linguistics syntax is a set of rules, principal, and process that govern the structure of a sentence in a given language\n",
        "* The term syntax is also used to refer the study of such pricipal and processess\n",
        "\n",
        "* **Syntax Tree** is a tree representation of syntatic structure of sentences or strings."
      ],
      "metadata": {
        "id": "BZnjFdzL-eDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install svgling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WaQcvpduAN0D",
        "outputId": "f286fe04-8104-4a4c-dca1-ffdbf7882f36"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting svgling\n",
            "  Downloading svgling-0.3.1-py3-none-any.whl (21 kB)\n",
            "Collecting svgwrite\n",
            "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 3.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
            "Successfully installed svgling-0.3.1 svgwrite-1.4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new = \"the big cat ate the little mouse who was after fresh cheese\"\n",
        "\n",
        "new_token = nltk.pos_tag(word_tokenize(new))\n",
        "print(new_token)"
      ],
      "metadata": {
        "id": "dBWv3PkX3Umy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "829bca63-ce41-47f4-953a-c14cf2203ed0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 'DT'), ('big', 'JJ'), ('cat', 'NN'), ('ate', 'VBD'), ('the', 'DT'), ('little', 'JJ'), ('mouse', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('after', 'IN'), ('fresh', 'JJ'), ('cheese', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grammer_np = r\"AA: {<DT><JJ><NN>}\"\n",
        "grammer_np = r\"AA: {<JJ><NN>}\"\n",
        "grammer_np = r\"AA: {<.*><JJ><NN>}\"\n",
        "\n",
        "grammer_np = r\"\"\"\n",
        "AA: {<DT><JJ>+}\n",
        "{<NN><VBD>+}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "_cE8-yex_en9"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_parser = nltk.RegexpParser(grammer_np)\n",
        "chunk_result = chunk_parser.parse(new_token)\n",
        "chunk_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        },
        "id": "StZK0nU4_yPe",
        "outputId": "46ba1521-bd71-48fa-8f9f-db0fe7ff80ca"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tree('S', [Tree('AA', [('the', 'DT'), ('big', 'JJ')]), Tree('AA', [('cat', 'NN'), ('ate', 'VBD')]), Tree('AA', [('the', 'DT'), ('little', 'JJ')]), ('mouse', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('after', 'IN'), ('fresh', 'JJ'), ('cheese', 'NN')])"
            ],
            "image/svg+xml": "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,576.0,168.0\" width=\"576px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"13.8889%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">AA</text></svg><svg width=\"50%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"50%\" x=\"50%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">big</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.94444%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"13.8889%\" x=\"13.8889%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">AA</text></svg><svg width=\"50%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cat</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"50%\" x=\"50%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ate</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20.8333%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"18.0556%\" x=\"27.7778%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">AA</text></svg><svg width=\"38.4615%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.2308%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"61.5385%\" x=\"38.4615%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">little</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"69.2308%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"36.8056%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"9.72222%\" x=\"45.8333%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">mouse</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50.6944%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.94444%\" x=\"55.5556%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">who</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">WP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"59.0278%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.94444%\" x=\"62.5%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">was</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.9722%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"9.72222%\" x=\"69.4444%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">after</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"74.3056%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"9.72222%\" x=\"79.1667%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">fresh</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"84.0278%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"11.1111%\" x=\"88.8889%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">cheese</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.4444%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TextBlob**"
      ],
      "metadata": {
        "id": "pTxMfHm4CvJG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob"
      ],
      "metadata": {
        "id": "Tmwk7T1WAAMp"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"John used to be very happy if no one ask him to work.\""
      ],
      "metadata": {
        "id": "gXfO2l9DC3QZ"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TextBlob(sent).words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMiMQ2RrDCPx",
        "outputId": "15d85bed-8551-40b3-cc06-a0807369da7b"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WordList(['John', 'used', 'to', 'be', 'very', 'happy', 'if', 'no', 'one', 'ask', 'him', 'to', 'work'])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TextBlob(sent).tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rC1UHWm8DGNL",
        "outputId": "848312f8-dff5-4ed1-fb07-6e8498c8b2ba"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('John', 'NNP'),\n",
              " ('used', 'VBD'),\n",
              " ('to', 'TO'),\n",
              " ('be', 'VB'),\n",
              " ('very', 'RB'),\n",
              " ('happy', 'JJ'),\n",
              " ('if', 'IN'),\n",
              " ('no', 'DT'),\n",
              " ('one', 'CD'),\n",
              " ('ask', 'VBZ'),\n",
              " ('him', 'PRP'),\n",
              " ('to', 'TO'),\n",
              " ('work', 'VB')]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TextBlob(sent).sentiment.polarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSSmJR8JDLWV",
        "outputId": "1cb06a34-ef7b-45fc-d667-ee02a6375848"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"John is very sad and upset today\"\n",
        "TextBlob(sent).sentiment.polarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD8sALwCDTlP",
        "outputId": "1f21056b-04db-4adc-d4d5-3a50f32859e2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.65"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"John is feeling nice today\"\n",
        "TextBlob(sent).sentiment.polarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5ICLS-aDfoI",
        "outputId": "52b785e2-82c5-4a62-b6aa-c4a4160d05c8"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"John is travelling today\"\n",
        "TextBlob(sent).sentiment.polarity"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfrcQ4dxDmxR",
        "outputId": "6cb5c7a0-36e2-46af-8774-e78e0c38d3f8"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0eKVC1HjDsh1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}